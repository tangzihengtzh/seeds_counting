import torch, os
from torch import nn, optim
from torch.utils.data import DataLoader
from mydatasets import SeedCountingDataset
from nets import TemplateMatchingNet        # æŠŠä¸Šé¢ç½‘ç»œå•ç‹¬æ”¾ model.py æ›´æ•´æ´
import os, torch, matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm
from my_loss import mixed_loss,GaussianDensityLoss,generalized_density_loss
import pandas as pd  # ç¡®ä¿å·²å¯¼å…¥ pandas






# ---------- å¯è°ƒå‚æ•° ----------
# data_root   = r"data_gen\train_data"
data_root   = r"E:\python_prj\2025_5_26\solid_bg_data"
val_root    = r"data_gen\val_data"
epochs      = 10
batch_size  = 1
lr          = 1e-4
device      = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("å½“å‰è®¾å¤‡ï¼š",device)

# ---------- æ•°æ® ----------
train_set = SeedCountingDataset(data_root,  canvas_size=(256,256), seed_resize=32)
print("è®­ç»ƒé›†æ•°é‡ï¼š",train_set.__len__())
val_set   = SeedCountingDataset(val_root,   canvas_size=(256,256), seed_resize=32)
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)
val_loader   = DataLoader(val_set,   batch_size=1,          shuffle=False)

# ---------- æ¨¡å‹ / æŸå¤± / ä¼˜åŒ– ----------
model = TemplateMatchingNet().to(device)
# criterion = nn.MSELoss()
# criterion = mixed_loss
criterion = generalized_density_loss
# criterion = GaussianDensityLoss(alpha=1.0, beta=1.0, gamma=0.5)
optimizer = optim.Adam(model.parameters(), lr=lr)

# ---------------- exp ç›®å½•è‡ªåŠ¨é€’å¢ ----------------
out_root = Path("train_out")
out_root.mkdir(exist_ok=True)
exp_idx = 1
while (out_root / f"exp{exp_idx}").exists():
    exp_idx += 1
exp_dir = out_root / f"exp{exp_idx}"
exp_dir.mkdir()
print(f"â¤ Results will be saved to {exp_dir}")

# ---------------- è®­ç»ƒå¾ªç¯ ----------------
train_mse_hist, val_mse_hist = [], []
best_val = float("inf")

for epoch in range(1, epochs + 1):
    print(f"\nEpoch {epoch}/{epochs}")
    model.train()
    running_loss = 0.0

    # ---------- Train ----------
    for comp, seed, den in tqdm(train_loader, desc="Train", unit="batch", leave=False):
        comp, seed, den = comp.to(device), seed.to(device), den.to(device)
        pred = model(comp, seed)
        if pred.shape[-2:] != den.shape[-2:]:
            pred = torch.nn.functional.interpolate(pred, size=den.shape[-2:],
                                                   mode="bilinear", align_corners=False)
        loss = criterion(pred, den)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * comp.size(0)

    train_mse = running_loss / len(train_set)
    train_mse_hist.append(train_mse)
    print(f"  train MSE: {train_mse:.4f}")

    # ---------- Val ----------
    model.eval()
    val_err = 0.0
    with torch.no_grad():
        for comp, seed, den in tqdm(val_loader, desc=" Val ", unit="batch", leave=False):
            comp, seed, den = comp.to(device), seed.to(device), den.to(device)
            pred = model(comp, seed)
            if pred.shape[-2:] != den.shape[-2:]:
                pred = torch.nn.functional.interpolate(pred, size=den.shape[-2:],
                                                       mode="bilinear", align_corners=False)
            val_err += criterion(pred, den).item()

    val_mse = val_err / len(val_loader)
    val_mse_hist.append(val_mse)
    print(f"  val   MSE: {val_mse:.4f}")

    # ---------- Checkpoint ----------
    if epoch % 5 == 0:
        ckpt_epoch = exp_dir / f"model_epoch{epoch}.pth"
        torch.save(model.state_dict(), ckpt_epoch)
        print(f"    ğŸ“Œ Checkpoint saved: {ckpt_epoch}")

    if val_mse < best_val:
        best_val = val_mse
        torch.save(model.state_dict(), exp_dir / "model_best.pth")

# ---------------- è®­ç»ƒç»“æŸ ----------------
ckpt_final = exp_dir / "model_last.pth"
torch.save(model.state_dict(), ckpt_final)
print(f"âœ”ï¸  Final model saved: {ckpt_final}")

# # ---------- ç»˜åˆ¶ Loss æ›²çº¿ ----------
# plt.figure(figsize=(6,4))
# plt.plot(train_mse_hist, label="Train MSE")
# plt.plot(val_mse_hist,   label="Val MSE")
# plt.xlabel("Epoch")
# plt.ylabel("MSE")
# plt.title("Training Curve")
# plt.legend()
# plt.grid(True)
# curve_path = exp_dir / "loss_curve.png"
# plt.savefig(curve_path, dpi=150)
# plt.close()
# print(f"ğŸ“ˆ Loss curve saved to {curve_path}")



# ---------- ç»˜åˆ¶ Loss æ›²çº¿ ----------
plt.figure(figsize=(6,4))
plt.plot(train_mse_hist, label="Train MSE")
plt.plot(val_mse_hist,   label="Val MSE")
plt.xlabel("Epoch")
plt.ylabel("MSE")
plt.title("Training Curve")
plt.legend()
plt.grid(True)
curve_path = exp_dir / "loss_curve.png"
plt.savefig(curve_path, dpi=150)
plt.close()
print(f"ğŸ“ˆ Loss curve saved to {curve_path}")

# ---------- ä¿å­˜ Loss æ•°æ®åˆ° Excel ----------
loss_data = pd.DataFrame({
    "Epoch": list(range(1, len(train_mse_hist)+1)),
    "Train MSE": train_mse_hist,
    "Val MSE": val_mse_hist
})
excel_path = exp_dir / "loss_curve.xlsx"
loss_data.to_excel(excel_path, index=False)
print(f"ğŸ“Š Loss data saved to {excel_path}")



# # ---------------- è®­ç»ƒå¾ªç¯ ----------------
# train_mse_hist, val_mse_hist = [], []
#
# for epoch in range(1, epochs + 1):
#     print(f"\nEpoch {epoch}/{epochs}")
#     model.train()
#     running_loss = 0.0
#
#     for comp, seed, den in tqdm(train_loader, desc="Train", unit="batch", leave=False):
#         comp, seed, den = comp.to(device), seed.to(device), den.to(device)
#
#         pred = model(comp, seed)
#         if pred.shape[-2:] != den.shape[-2:]:
#             pred = torch.nn.functional.interpolate(pred, size=den.shape[-2:], mode='bilinear', align_corners=False)
#
#         # loss,_ = criterion(pred, den)
#         loss = criterion(pred, den)
#
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         running_loss += loss.item() * comp.size(0)
#
#     train_mse = running_loss / len(train_set)
#     train_mse_hist.append(train_mse)
#     print(f"  train MSE: {train_mse:.4f}")
#
#     # ---------- éªŒè¯ ----------
#     model.eval()
#     val_err = 0.0
#     with torch.no_grad():
#         for comp, seed, den in tqdm(val_loader, desc=" Val ", unit="batch", leave=False):
#             comp, seed, den = comp.to(device), seed.to(device), den.to(device)
#             pred = model(comp, seed)
#             if pred.shape[-2:] != den.shape[-2:]:
#                 pred = torch.nn.functional.interpolate(pred, size=den.shape[-2:], mode='bilinear', align_corners=False)
#             val_err += criterion(pred, den).item()
#             # val_err += criterion(pred, den)[0].item()
#
#     val_mse = val_err / len(val_loader)
#     val_mse_hist.append(val_mse)
#     print(f"  val   MSE: {val_mse:.4f}")
#
# # ---------------- ä¿å­˜æƒé‡ ----------------
# ckpt_path = exp_dir / "model.pth"
# torch.save(model.state_dict(), ckpt_path)
# print(f"âœ”ï¸  Model saved: {ckpt_path}")
#
# # ---------------- ç»˜åˆ¶ & ä¿å­˜ loss æ›²çº¿ ----------------
# plt.figure(figsize=(6,4))
# plt.plot(train_mse_hist, label="Train MSE")
# plt.plot(val_mse_hist,   label="Val MSE")
# plt.xlabel("Epoch")
# plt.ylabel("MSE")
# plt.title("Training Curve")
# plt.legend()
# plt.grid(True, alpha=0.3)
# plt.tight_layout()
# curve_path = exp_dir / "loss_curve.png"
# plt.savefig(curve_path, dpi=300)
# plt.close()
# print(f"âœ”ï¸  Loss curve saved: {curve_path}")
#
# # ï¼ˆå¯é€‰ï¼‰è®°å½•åˆ° txt
# with open(exp_dir / "train_log.txt", "w") as f:
#     for ep, (tr, va) in enumerate(zip(train_mse_hist, val_mse_hist), 1):
#         f.write(f"Epoch {ep}\ttrain={tr:.6f}\tval={va:.6f}\n")
# print("å…¨éƒ¨å®Œæˆï¼")










# # ---------- è®­ç»ƒå¾ªç¯ ----------
# for epoch in range(1, epochs+1):
#     print("epochï¼š",epochs)
#     model.train()
#     running_loss = 0.0
#     for comp, seed, den in tqdm(train_loader, desc="Train", unit="batch", leave=False):
#         # print("è·å–æ•°æ®æ¡ç›®ï¼š",comp.shape, seed.shape, den.shape)
#         # exit(1)
#         comp, seed, den = comp.to(device), seed.to(device), den.to(device)
#
#         # pred = model(comp, seed)          # [B,1,H,W]
#         # print("å½“å‰è®¡ç®—ç»“æœï¼š",pred.shape)
#         # loss = criterion(pred, den)
#
#         pred = model(comp, seed)  # [B,1,*,*]
#         if pred.shape[-2:] != den.shape[-2:]:
#             pred = torch.nn.functional.interpolate(
#                 pred, size=den.shape[-2:], mode='bilinear', align_corners=False)
#         loss = criterion(pred, den)
#
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#         running_loss += loss.item() * comp.size(0)
#
#     avg_loss = running_loss / len(train_set)
#     print(f"Epoch {epoch}/{epochs}  train MSE: {avg_loss:.4f}")
#
#     # ---- ç®€å•éªŒè¯ ----
#     model.eval()
#     with torch.no_grad():
#         val_err = 0.0
#         for comp, seed, den in val_loader:
#             comp, seed, den = comp.to(device), seed.to(device), den.to(device)
#             pred = model(comp, seed)  # [B,1,*,*]
#             if pred.shape[-2:] != den.shape[-2:]:
#                 pred = torch.nn.functional.interpolate(
#                     pred, size=den.shape[-2:], mode='bilinear', align_corners=False)
#             val_err += criterion(pred, den).item()
#     print(f"               val MSE : {val_err/len(val_loader):.4f}")
#
# # ---------- ä¿å­˜ ----------
# torch.save(model.state_dict(), "template_match_net.pth")
